{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### classify.py\n",
    "\n",
    "classify.py: This should classify your data along any dimension of your choosing (e.g., sentiment, gender, spam, etc.). You may write any files you need to save the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "from TwitterAPI import TwitterAPI, TwitterOAuth, TwitterRestPager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import collect as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter():\n",
    "    \"\"\" Construct an instance of TwitterAPI using the tokens you entered above.\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    o = TwitterOAuth.read_file('credentials.txt')\n",
    "    # Using OAuth1...\n",
    "    twitter = TwitterAPI(o.consumer_key,\n",
    "                 o.consumer_secret,\n",
    "                 o.access_token_key,\n",
    "                 o.access_token_secret)\n",
    "    return twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
    "\n",
    "def get_afinn():\n",
    "    url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "\n",
    "    afinn = dict()\n",
    "\n",
    "    for line in afinn_file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            afinn[parts[0].decode(\"utf-8\")] = int(parts[1])\n",
    "\n",
    "    print('read %d AFINN terms.\\nE.g.: %s' % (len(afinn), str(list(afinn.items())[:10])))\n",
    "    return afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# afinn = get_afinn()\n",
    "def afinn_sentiment(terms, afinn):\n",
    "    total = 0.\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "#             print('\\t%s=%d' % (t, afinn[t]))\n",
    "            total += afinn[t]\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_park_list(filename, top = 20):\n",
    "    f = open(filename)\n",
    "    da = f.readlines()\n",
    "#     data = da.split('\\n')\n",
    "    data = [l.strip('\\n') for l in da]\n",
    "    return data[:20]\n",
    "\n",
    "# fn = 'save_trends.txt'    \n",
    "# park_list = get_park_list(fn)\n",
    "# print (park_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_helper(name, max_len=200):\n",
    "    twitter = get_twitter()\n",
    "    tweets = []\n",
    "    m_id = 0 if len(tweets) is 0 else tweets[len(tweets)-1]['id']\n",
    "    quary = name+' national park OR \"'+name+'nationalpark\"-filter:retweets'\n",
    "    while len(tweets)<max_len:\n",
    "        rid = m_id\n",
    "        m_id = 0 if len(tweets) is 0 else tweets[len(tweets)-1]['id']\n",
    "        for r in twitter.request('search/tweets', {'q':  quary, 'country': 'United States', 'lang': 'en',\n",
    "                                                   'count':100, 'max_id': m_id}):\n",
    "\n",
    "            if r['id'] == rid:\n",
    "                print('Read %d tweets for %s' % (len(tweets),name))\n",
    "                return tweets\n",
    "#             tweets.append(r['text'].replace('\\n', ' ').lower())\n",
    "            tweets.append(r)\n",
    "        \n",
    "    print('Read %d tweets for %s' % (len(tweets),name))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(park_list):\n",
    "    park_tweets =  defaultdict()\n",
    "    num_tweets = 0\n",
    "    fw = open('class.txt', 'a')\n",
    "    for name in park_list:\n",
    "        t = get_tweets_helper(name)\n",
    "        park_tweets[name] = t\n",
    "        num_tweets += len(t)\n",
    "        fw.write('\\n%d\\t%s\\t'%(len(t),name))\n",
    "        print(t[0],file=fw)\n",
    "        \n",
    "    return (park_tweets, num_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_score_from_new_tweets(afinn, park_list,park_tweets):\n",
    "    park_afinn = defaultdict(lambda: 0)\n",
    "    for n in park_list:\n",
    "        tweets = park_tweets[n]\n",
    "        if len(tweets) is not 0:\n",
    "            score = 0\n",
    "            for t in tweets:\n",
    "                score += afinn_sentiment(t['text'].split(),afinn)\n",
    "            park_afinn[n] = score/len(tweets)\n",
    "    return sorted(park_afinn.items(), key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_score(park_list, parks):\n",
    "    park_afinn = defaultdict(lambda: 0)\n",
    "    for n in park_list[:20]:\n",
    "        tweets = parks[n]\n",
    "        if len(tweets) is not 0:\n",
    "            score = 0\n",
    "            for t in tweets:\n",
    "                score += afinn_sentiment(t.split(),afinn)\n",
    "            park_afinn[n] = score/len(tweets)\n",
    "    return sorted(park_afinn.items(), key = lambda x:x[1], reverse=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_nums(fn, nums):\n",
    "    rawData = open(fn, 'a')\n",
    "    rawData.write('%d\\n'%nums)\n",
    "    print('write %d to %s' % (nums,fn))\n",
    "    rawData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print('Read AFINN file:')\n",
    "# # afinn = get_afinn()\n",
    "\n",
    "# def main():\n",
    "#     print('Read AFINN file:')\n",
    "#     afinn = get_afinn()\n",
    "# #     print('\\nModel 1: Using data collected before:')\n",
    "# #     p_nums, parks = get_data()\n",
    "# #     park_list = get_list(p_nums)[:10]\n",
    "# #     result = get_score(park_list, parks)\n",
    "# #     print('The top 10 recommended national parks are:\\n')\n",
    "# #     for r in result:\n",
    "# #         print(' score:%f, %s national park'%(r[1], r[0]))\n",
    "#     park_list = get_park_list('save_trends.txt')\n",
    "#     print('\\nUsing new tweets collected from Twitter:')\n",
    "#     park_tweets, num_tweets= get_tweets(park_list)\n",
    "#     print('Get %d tweets using park name as key word:' % num_tweets)\n",
    "#     save_nums('num_tweets.txt', num_tweets)\n",
    "#     result2 = get_score_from_new_tweets(afinn,park_list,park_tweets)\n",
    "#     print('The top 10 recommended national parks are:\\n')\n",
    "#     for r in result2:\n",
    "#         print(' score:%f, %s national park'%(r[1], r[0]))\n",
    "        \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read AFINN file:\n",
      "read 2462 AFINN terms.\n",
      "E.g.: [('warn', -2), ('worn', -1), ('dreams', 1), ('agonises', -3), ('murder', -2), ('stressors', -2), ('worsen', -3), ('disabling', -1), ('swear', -2), ('earnest', 2)]\n",
      "\n",
      "Using new tweets collected from Twitter:\n",
      "Read 200 tweets for yosemite\n",
      "Read 297 tweets for zion\n",
      "Get 497 tweets using park name as key word:\n",
      "write 497 to num_tweets.txt\n"
     ]
    }
   ],
   "source": [
    "print('Read AFINN file:')\n",
    "afinn = get_afinn()\n",
    "park_list = get_park_list('save_trends.txt')\n",
    "print('\\nUsing new tweets collected from Twitter:')\n",
    "park_tweets, num_tweets= get_tweets(park_list)\n",
    "print('Get %d tweets using park name as key word:' % num_tweets)\n",
    "save_nums('num_tweets.txt', num_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0987d60253f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_score_from_new_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafinn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpark_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The top 10 recommended national parks are:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' score:%f, %s national park'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-be6378843e4e>\u001b[0m in \u001b[0;36mget_score_from_new_tweets\u001b[0;34m(afinn, park_list, park_tweets)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpark_afinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpark_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpark_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result2 = get_score_from_new_tweets(afinn,park_list,num_tweets)\n",
    "print('The top 10 recommended national parks are:\\n')\n",
    "for r in result2:\n",
    "    print(' score:%f, %s national park'%(r[1], r[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "for n in park_list:\n",
    "    tweets = park_tweets[n]\n",
    "    print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_tweets['yosemite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
