{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect.py\n",
    "\n",
    "This should collect data used in your analysis. This may mean submitting queries to Twitter or Facebook API, or scraping webpages. The data should be raw and come directly from the original source -- that is, you may not use data that others have already collected and processed for you (e.g., you may not use SNAP datasets). Running this script should create a file or files containing the data that you need for the subsequent phases of analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "from TwitterAPI import TwitterAPI, TwitterOAuth, TwitterRestPager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import collect as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter():\n",
    "    \"\"\" Construct an instance of TwitterAPI using the tokens you entered above.\n",
    "    Returns:\n",
    "      An instance of TwitterAPI.\n",
    "    \"\"\"\n",
    "    o = TwitterOAuth.read_file('credentials.txt')\n",
    "    # Using OAuth1...\n",
    "    twitter = TwitterAPI(o.consumer_key,\n",
    "                 o.consumer_secret,\n",
    "                 o.access_token_key,\n",
    "                 o.access_token_secret)\n",
    "    return twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def robust_request(twitter, resource, params, max_tries=5):\n",
    "    \"\"\" If a Twitter request fails, sleep for 15 minutes.\n",
    "    Do this at most max_tries times before quitting.\n",
    "    Args:\n",
    "      twitter .... A TwitterAPI object.\n",
    "      resource ... A resource string to request; e.g., \"friends/ids\"\n",
    "      params ..... A parameter dict for the request, e.g., to specify\n",
    "                   parameters like screen_name or count.\n",
    "      max_tries .. The maximum number of tries to attempt.\n",
    "    Returns:\n",
    "      A TwitterResponse object, or None if failed.\n",
    "    \"\"\"\n",
    "    for i in range(max_tries):\n",
    "        request = twitter.request(resource, params)\n",
    "        if request.status_code == 200:\n",
    "            return request\n",
    "        else:\n",
    "            print('Got error %s \\nsleeping for 15 minutes.' % request.text)\n",
    "            sys.stderr.flush()\n",
    "            time.sleep(61 * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets( max_len = 5000):\n",
    "    \n",
    "    twitter = get_twitter()\n",
    "    tweets = []\n",
    "    m_id = 0 if len(tweets) is 0 else tweets[len(tweets)-1]['id']\n",
    "    while len(tweets)<max_len:\n",
    "        rid = m_id\n",
    "        m_id = 0 if len(tweets) is 0 else tweets[len(tweets)-1]['id']\n",
    "        for r in robust_request(twitter,'search/tweets',{'q':  'nationalpark OR \"national park\" -filter:retweets', \n",
    "                                                   'country': 'United States', 'lang': 'en',\n",
    "                                                   'count':100, 'max_id': m_id}):\n",
    "#         for r in twitter.request('search/tweets', {'q':  'nationalpark OR \"national park\" -filter:retweets', \n",
    "#                                                    'country': 'United States', 'lang': 'en',\n",
    "#                                                    'count':100, 'max_id': m_id}):\n",
    "\n",
    "            if r['id'] == rid:\n",
    "                return (tweets,len(tweets)) \n",
    "            tweets.append(r)\n",
    "        if len(tweets)%100 == 0:\n",
    "            print('read %d tweets' % len(tweets) )\n",
    "    return (tweets,len(tweets))    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_tweets2file(tweets, write_type = 'a', filename='save_tweets.txt',):\n",
    "    rawData = open(filename, write_type)\n",
    "    for t in tweets:\n",
    "        rawData.write(t['text'].replace('\\n', ' ').lower()+'\\n')\n",
    "    print('write %d tweets to %s' % (len(tweets),filename))\n",
    "    rawData.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data_from_file(filename = 'rawtweets.txt'):\n",
    "    d = open(filename)\n",
    "    f = d.readlines()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_national_park_list():\n",
    "    f = open(\"national_park_list.txt\")\n",
    "    lines = f.readlines()\n",
    "    park_list = [l.rstrip('\\r\\n').lower() for l in lines]\n",
    "    return park_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_multi_names(park_names):\n",
    "    return [(n, ''.join(n.split()), n.replace(' ', '_')) if ' ' in n else (n,) for n in park_names ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_park_trend(park_list, datatable):\n",
    "    parks = defaultdict(lambda: [])\n",
    "    for n in park_list:\n",
    "        for i,t in enumerate(datatable):\n",
    "            if n in t.lower():\n",
    "                parks[n].append(t.lower())\n",
    "                datatable.remove(t)\n",
    "    park_nums = defaultdict(lambda: 0)\n",
    "    for n in park_list:\n",
    "        park_nums[n]=len(parks[n])\n",
    "        \n",
    "    p_nums =sorted(park_nums.items(), key=lambda x: x[1],reverse=True)\n",
    "    return (p_nums, parks)\n",
    "\n",
    "\n",
    "def write_trends2file(p_nums,fileName='save_trends.txt'):\n",
    "    with open('save_trends.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join(r[0] for r in p_nums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_nums(fn, nums):\n",
    "    rawData = open(fn, 'w')\n",
    "    rawData.write('%d\\n'%nums)\n",
    "    print('write %d number to %s' % (nums,fn))\n",
    "    rawData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data2commu(tweets):\n",
    "    userdata = defaultdict(lambda:0)\n",
    "    for t in tweets:\n",
    "        if t['user']['screen_name'] not in userdata:\n",
    "            userdata[t['user']['screen_name']] = t['user']['friends_count']\n",
    "    userdata = sorted(userdata.items(), key=lambda x: x[1],reverse=True)[:14]\n",
    "    with open('users.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join(u[0] for u in userdata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write 5082 tweets to rawdata.txt\n",
      "write 5082 number to num_tweets.txt\n",
      "read 5082 tweets\n",
      "the last 10 tweets are:\n",
      "\n",
      "â€¦filled my day with joy. And in total I encountered 5 people on the trail my entire day (almost 7 hours!) at a famous National Park.\n",
      "\n",
      "Zion national park. https://t.co/3uIw1F33d4\n",
      "\n",
      "Meh. I guess you could say the view was pretty grand. ðŸ˜œ @ Grand Canyon National Park https://t.co/6XmfFYsw3U\n",
      "\n",
      "Doesn't take a  scientist to see the obvious glacial recession in places like Glacier Bay National Park. I recommenâ€¦ https://t.co/JhLZDZZOkr\n",
      "\n",
      "I must have landed on Mars\n",
      "THIS.IS.UNBELIEVABLE!!!!!\n",
      "#grandcanyon @ Grand Canyon National Park -â€¦ https://t.co/zTf5NypPGO\n",
      "\n",
      "Shot this near Hannegan Peak in North Cascades National Park, WA [4032 x 3024] https://t.co/nmDCKItqPH\n",
      "\n",
      "That's why I love this place! @lifeapp_ @ Burleigh Heads National Park Track https://t.co/lBp7HCHgAX\n",
      "\n",
      "Shot this near Hannegan Peak in North Cascades National Park, WA [4032 x 3024] via /r/Eartâ€¦ https://t.co/rgSVg7Vpdl https://t.co/iYzPwso6Ip\n",
      "\n",
      "Just posted a photo @ Yosemite National Park https://t.co/GeeIWq5S8x\n",
      "\n",
      "Creepiest Places in U.S. National Parks: These national park locations have mysterious and creepy local legends,â€¦ https://t.co/1BEUwRn7tp\n",
      "\n",
      "5086 number of data got from rawdata.txt\n",
      "Number of national parks:  59\n",
      "The number of US. national parks mentioned in the passed 1 week:\n",
      "1 ('yosemite', 271)\n",
      "2 ('zion', 182)\n",
      "3 ('yellowstone', 168)\n",
      "4 ('glacier', 136)\n",
      "5 ('grand canyon', 113)\n",
      "6 ('joshua tree', 104)\n",
      "7 ('great smoky mountains', 96)\n",
      "8 ('rocky mountain', 93)\n",
      "9 ('arches', 86)\n",
      "10 ('olympic', 69)\n",
      "11 ('sequoia', 54)\n",
      "12 ('acadia', 39)\n",
      "13 ('shenandoah', 38)\n",
      "14 ('badlands', 36)\n",
      "15 ('big bend', 35)\n",
      "16 ('bryce canyon', 33)\n",
      "17 ('grand teton', 32)\n",
      "18 ('everglades', 28)\n",
      "19 ('death valley', 28)\n",
      "20 ('redwood', 27)\n",
      "21 ('hot springs', 24)\n",
      "22 ('mount rainier', 22)\n",
      "23 ('crater lake', 22)\n",
      "24 ('canyonlands', 22)\n",
      "25 ('saguaro', 20)\n",
      "26 ('kings canyon', 18)\n",
      "27 ('denali', 16)\n",
      "28 ('great sand dunes', 15)\n",
      "29 ('north cascades', 14)\n",
      "30 ('biscayne', 12)\n",
      "31 ('isle royale', 12)\n",
      "32 ('mesa verde', 11)\n",
      "33 ('dry tortugas', 11)\n",
      "34 ('lassen volcanic', 11)\n",
      "35 ('voyageurs', 11)\n",
      "36 ('glacier bay', 9)\n",
      "37 ('carlsbad caverns', 8)\n",
      "38 ('black canyon of the gunnison', 8)\n",
      "39 ('guadalupe mountains', 8)\n",
      "40 ('capitol reef', 8)\n",
      "41 ('american samoa', 8)\n",
      "42 ('theodore roosevelt', 7)\n",
      "43 ('petrified forest', 6)\n",
      "44 ('mammoth cave', 6)\n",
      "45 ('great basin', 6)\n",
      "46 ('congaree', 5)\n",
      "47 ('channel islands', 5)\n",
      "48 ('cuyahoga valley', 5)\n",
      "49 ('hawaii volcanoes', 4)\n",
      "50 ('katmai', 4)\n",
      "51 ('kenai fjords', 3)\n",
      "52 ('lake clark', 3)\n",
      "53 ('virgin islands', 3)\n",
      "54 ('haleakala', 2)\n",
      "55 ('gates of the arctic', 1)\n",
      "56 ('wind cave', 1)\n",
      "57 ('kobuk valley', 0)\n",
      "58 ('wrangell - st. elias', 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    twitter = get_twitter()\n",
    "    tweets,num_tweets = get_tweets() \n",
    "    fn = 'rawdata.txt'\n",
    "    data2commu(tweets)\n",
    "    write_tweets2file(tweets,write_type = 'w',filename = fn)\n",
    "    save_nums('num_tweets.txt', len(tweets))\n",
    "    print('read %d tweets' % len(tweets) )\n",
    "    print('the last 10 tweets are:\\n')\n",
    "    for t in tweets[-10:]:\n",
    "        print(t['text']+'\\n')\n",
    "    #read data from file in case of Twitter API rate limit\n",
    "    datatable = get_data_from_file(filename = fn)\n",
    "    print('%s number of data got from %s'%(len(datatable),fn))\n",
    "    park_list = get_national_park_list()\n",
    "    print('Number of national parks: ', len(park_list))\n",
    "    p_nums, parks = get_park_trend(park_list, datatable)\n",
    "    write_trends2file(p_nums)\n",
    "    print('The number of US. national parks mentioned in the passed 1 week:')\n",
    "    for i, r in enumerate(p_nums):\n",
    "        print (i+1, r)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
