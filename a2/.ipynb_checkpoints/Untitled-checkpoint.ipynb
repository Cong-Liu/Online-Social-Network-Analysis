{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# No imports allowed besides these.\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import string\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\" Download and unzip data.\n",
    "    DONE ALREADY.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/xk4glpk61q3qrg2/imdb.tgz?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'imdb.tgz')\n",
    "    tar = tarfile.open(\"imdb.tgz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Walks all subdirectories of this path and reads all\n",
    "    the text files and labels.\n",
    "    DONE ALREADY.\n",
    "\n",
    "    Params:\n",
    "      path....path to files\n",
    "    Returns:\n",
    "      docs.....list of strings, one per document\n",
    "      labels...list of ints, 1=positive, 0=negative label.\n",
    "               Inferred from file path (i.e., if it contains\n",
    "               'pos', it is 1, else 0)\n",
    "    \"\"\"\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'pos', '*.txt'))])\n",
    "    data = [(1, open(f).readlines()[0]) for f in sorted(fnames)]\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'neg', '*.txt'))])\n",
    "    data += [(0, open(f).readlines()[0]) for f in sorted(fnames)]\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    return np.array([d[1] for d in data]), np.array([d[0] for d in data])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(doc, keep_internal_punct=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string.\n",
    "    The string should be converted to lowercase.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens.\n",
    "\n",
    "    Params:\n",
    "      doc....a string.\n",
    "      keep_internal_punct...see above\n",
    "    Returns:\n",
    "      a numpy array containing the resulting tokens.\n",
    "\n",
    "    >>> tokenize(\" Hi there! Isn't this fun?\", keep_internal_punct=False)\n",
    "    array(['hi', 'there', 'isn', 't', 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "    >>> tokenize(\"Hi there! Isn't this fun? \", keep_internal_punct=True)\n",
    "    array(['hi', 'there', \"isn't\", 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    if keep_internal_punct:\n",
    "        return np.array(re.sub(r'[^A-Za-z0-9_\\']',' ', doc.lower()).split())\n",
    "    return np.array(re.sub('\\W+', ' ', doc.lower()).split())\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_features(tokens, feats):\n",
    "    \"\"\"\n",
    "    Add features for each token. The feature name\n",
    "    is pre-pended with the string \"token=\".\n",
    "    Note that the feats dict is modified in place,\n",
    "    so there is no return value.\n",
    "\n",
    "    Params:\n",
    "      tokens...array of token strings from a document.\n",
    "      feats....dict from feature name to frequency\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> token_features(['hi', 'there', 'hi'], feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('token=hi', 2), ('token=there', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    # tokendic = defaultdict(lambda: 0)\n",
    "    for t in tokens:\n",
    "        feats['token=%s' % t] += 1\n",
    "#         key = 'token='+t\n",
    "#         feats[key] += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_pair_features(tokens, feats, k=3):\n",
    "    \"\"\"\n",
    "    Compute features indicating that two words occur near\n",
    "    each other within a window of size k.\n",
    "\n",
    "    For example [a, b, c, d] with k=3 will consider the\n",
    "    windows: [a,b,c], [b,c,d]. In the first window,\n",
    "    a_b, a_c, and b_c appear; in the second window,\n",
    "    b_c, c_d, and b_d appear. This example is in the\n",
    "    doctest below.\n",
    "    Note that the order of the tokens in the feature name\n",
    "    matches the order in which they appear in the document.\n",
    "    (e.g., a__b, not b__a)\n",
    "\n",
    "    Params:\n",
    "      tokens....array of token strings from a document.\n",
    "      feats.....a dict from feature to value\n",
    "      k.........the window size (3 by default)\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> token_pair_features(np.array(['a', 'b', 'c', 'd']), feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('token_pair=a__b', 1), ('token_pair=a__c', 1), ('token_pair=b__c', 2), ('token_pair=b__d', 1), ('token_pair=c__d', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "\n",
    "    tokens = sorted(tokens)\n",
    "    windows = []\n",
    "    token_pair = []\n",
    "    for i in range(len(tokens)-k+1):\n",
    "        windows.append(tokens[i:i+k])\n",
    "    for w in windows:\n",
    "        token_pair += list(combinations(w,2))\n",
    "    token_pair_count = Counter(token_pair)\n",
    "    for t in token_pair_count.keys():\n",
    "        key = 'token_pair='+t[0]+'__'+t[1]\n",
    "        feats[key] = token_pair_count[t]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_words = set(['bad', 'hate', 'horrible', 'worst', 'boring'])\n",
    "pos_words = set(['awesome', 'amazing', 'best', 'good', 'great', 'love', 'wonderful'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexicon_features(tokens, feats):\n",
    "    \"\"\"\n",
    "    Add features indicating how many time a token appears that matches either\n",
    "    the neg_words or pos_words (defined above). The matching should ignore\n",
    "    case.\n",
    "\n",
    "    Params:\n",
    "      tokens...array of token strings from a document.\n",
    "      feats....dict from feature name to frequency\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "\n",
    "    In this example, 'LOVE' and 'great' match the pos_words,\n",
    "    and 'boring' matches the neg_words list.\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> lexicon_features(np.array(['i', 'LOVE', 'this', 'great', 'boring', 'movie']), feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('neg_words', 1), ('pos_words', 2)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    cn = 0\n",
    "    cp = 0\n",
    "    for t in tokens:\n",
    "        t = t.lower()\n",
    "        if t in neg_words:\n",
    "            cn += 1\n",
    "        elif t in pos_words:\n",
    "            cp += 1\n",
    "    feats['neg_words'] = cn\n",
    "    feats['pos_words'] = cp\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featurize(tokens, feature_fns):\n",
    "    \"\"\"\n",
    "    Compute all features for a list of tokens from\n",
    "    a single document.\n",
    "\n",
    "    Params:\n",
    "      tokens........array of token strings from a document.\n",
    "      feature_fns...a list of functions, one per feature\n",
    "    Returns:\n",
    "      list of (feature, value) tuples, SORTED alphabetically\n",
    "      by the feature name.\n",
    "\n",
    "    >>> feats = featurize(np.array(['i', 'LOVE', 'this', 'great', 'movie']), [token_features, lexicon_features])\n",
    "    >>> feats\n",
    "    [('neg_words', 0), ('pos_words', 2), ('token=LOVE', 1), ('token=great', 1), ('token=i', 1), ('token=movie', 1), ('token=this', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    result = []\n",
    "\n",
    "    for function in feature_fns:\n",
    "        feats = defaultdict(lambda: 0)\n",
    "        function(tokens, feats)     \n",
    "        result.extend(feats.items())\n",
    "    return sorted(result, key=lambda x:x[0])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    \"\"\"\n",
    "    Given the tokens for a set of documents, create a sparse\n",
    "    feature matrix, where each row represents a document, and\n",
    "    each column represents a feature.\n",
    "\n",
    "    Params:\n",
    "      tokens_list...a list of lists; each sublist is an\n",
    "                    array of token strings from a document.\n",
    "      feature_fns...a list of functions, one per feature\n",
    "      min_freq......Remove features that do not appear in\n",
    "                    at least min_freq different documents.\n",
    "    Returns:\n",
    "      - a csr_matrix: See https://goo.gl/f5TiF1 for documentation.\n",
    "      This is a sparse matrix (zero values are not stored).\n",
    "      - vocab: a dict from feature name to column index. NOTE\n",
    "      that the columns are sorted alphabetically (so, the feature\n",
    "      \"token=great\" is column 0 and \"token=horrible\" is column 1\n",
    "      because \"great\" < \"horrible\" alphabetically),\n",
    "\n",
    "    >>> docs = [\"Isn't this movie great?\", \"Horrible, horrible movie\"]\n",
    "    >>> tokens_list = [tokenize(d) for d in docs]\n",
    "    >>> feature_fns = [token_features]\n",
    "    >>> X, vocab = vectorize(tokens_list, feature_fns, min_freq=1)\n",
    "    >>> type(X)\n",
    "    <class 'scipy.sparse.csr.csr_matrix'>\n",
    "    >>> X.toarray()\n",
    "    array([[1, 0, 1, 1, 1, 1],\n",
    "           [0, 2, 0, 1, 0, 0]], dtype=int64)\n",
    "    >>> sorted(vocab.items(), key=lambda x: x[1])\n",
    "    [('token=great', 0), ('token=horrible', 1), ('token=isn', 2), ('token=movie', 3), ('token=t', 4), ('token=this', 5)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    featurize_list = []\n",
    "    freq = Counter()\n",
    "    for tokens in tokens_list:\n",
    "#         print ('tokens', type(tokens), tokens)\n",
    "        one_list = featurize(tokens, feature_fns)\n",
    "\n",
    "        featurize_list.append(one_list)\n",
    "        freq.update(l[0] for l in one_list)\n",
    "        # indptr.append(len(one_list))\n",
    "#     print('featurize_list ', featurize_list)\n",
    "#     print ('count of tokens',freq.items())\n",
    "    if vocab is None:\n",
    "        vocab = {}\n",
    "        for f in sorted(freq.items()):\n",
    "#             print ('f[0]', f[0], 'f[1]',f[1])\n",
    "            if f[1] >= min_freq:\n",
    "                vocab[f[0]] = len(vocab)\n",
    "\n",
    "#     for k in voc.keys():\n",
    "#         data = [doc for doc in featurize_list]\n",
    "#     print ('voc', vocab.items())\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    for doc_num, doc in enumerate(featurize_list):\n",
    "        doc = [d for d in doc if d[0] in vocab]\n",
    "        data.extend([d[1] for d in doc])\n",
    "        rows.extend([doc_num] * len(doc))\n",
    "        cols.extend([vocab[d[0]] for d in doc])\n",
    "    cmtrx = csr_matrix((data, (rows, cols)), shape=(len(tokens_list), len(vocab)), dtype = int)\n",
    "    return cmtrx, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(truth, predicted):\n",
    "    \"\"\" Compute accuracy of predictions.\n",
    "    DONE ALREADY\n",
    "    Params:\n",
    "      truth.......array of true labels (0 or 1)\n",
    "      predicted...array of predicted labels (0 or 1)\n",
    "    \"\"\"\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)\n",
    "\n",
    "\n",
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    \"\"\"\n",
    "    Compute the average testing accuracy over k folds of cross-validation. You\n",
    "    can use sklearn's KFold class here (no random seed, and no shuffling\n",
    "    needed).\n",
    "\n",
    "    Params:\n",
    "      clf......A LogisticRegression classifier.\n",
    "      X........A csr_matrix of features.\n",
    "      labels...The true labels for each instance in X\n",
    "      k........The number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "      The average testing accuracy of the classifier\n",
    "      over each fold of cross-validation.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "#     cv = \n",
    "    accuracies = []\n",
    "    for train_ind, test_ind in KFold(len(labels), k):\n",
    "        clf.fit(X[train_ind], labels[train_ind])\n",
    "        predictions = clf.predict(X[test_ind])\n",
    "        accuracies.append(accuracy_score(labels[test_ind], predictions))\n",
    "\n",
    "    return np.mean(accuracies)\n",
    "    pass\n",
    "\n",
    "\n",
    "def combined_f(feature_fns):\n",
    "    combine = [combinations(feature_fns, n + 1) for n in range(len(feature_fns))]\n",
    "    return chain.from_iterable(combine)\n",
    "\n",
    "\n",
    "def eval_all_combinations(docs, labels, punct_vals,\n",
    "                          feature_fns, min_freqs):\n",
    "    \"\"\"\n",
    "    Enumerate all possible classifier settings and compute the\n",
    "    cross validation accuracy for each setting. We will use this\n",
    "    to determine which setting has the best accuracy.\n",
    "\n",
    "    For each setting, construct a LogisticRegression classifier\n",
    "    and compute its cross-validation accuracy for that setting.\n",
    "\n",
    "    In addition to looping over possible assignments to\n",
    "    keep_internal_punct and min_freqs, we will enumerate all\n",
    "    possible combinations of feature functions. So, if\n",
    "    feature_fns = [token_features, token_pair_features, lexicon_features],\n",
    "    then we will consider all 7 combinations of features (see Log.txt\n",
    "    for more examples).\n",
    "\n",
    "    Params:\n",
    "      docs..........The list of original training documents.\n",
    "      labels........The true labels for each training document (0 or 1)\n",
    "      punct_vals....List of possible assignments to\n",
    "                    keep_internal_punct (e.g., [True, False])\n",
    "      feature_fns...List of possible feature functions to use\n",
    "      min_freqs.....List of possible min_freq values to use\n",
    "                    (e.g., [2,5,10])\n",
    "\n",
    "    Returns:\n",
    "      A list of dicts, one per combination. Each dict has\n",
    "      four keys:\n",
    "      'punct': True or False, the setting of keep_internal_punct\n",
    "      'features': The list of functions used to compute features.\n",
    "      'min_freq': The setting of the min_freq parameter.\n",
    "      'accuracy': The average cross_validation accuracy for this setting, using 5 folds.\n",
    "\n",
    "      This list should be SORTED in descending order of accuracy.\n",
    "\n",
    "      This function will take a bit longer to run (~20s for me).\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    result = []\n",
    "    tokens_list = []\n",
    "    for punct in punct_vals:\n",
    "        tokens_list=[tokenize(d, punct) for d in docs]\n",
    "        for minf in min_freqs:\n",
    "            feature_chains = combined_f(feature_fns)\n",
    "            for feature_list in feature_chains:\n",
    "                X, vocab = vectorize(tokens_list, feature_list, minf)\n",
    "                accuracy = cross_validation_accuracy(LogisticRegression(), X, labels, 5)\n",
    "                result.append({'punct': punct,\n",
    "                               'features': feature_list,\n",
    "                               'min_freq':minf,\n",
    "                               'accuracy': accuracy})\n",
    "\n",
    "    return sorted(result, key=lambda x: x['accuracy'])[::-1]\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_sorted_accuracies(results):\n",
    "    \"\"\"\n",
    "    Plot all accuracies from the result of eval_all_combinations\n",
    "    in ascending order of accuracy.\n",
    "    Save to \"accuracies.png\".\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.plot(sorted(x['accuracy'] for x in results))\n",
    "    plt.xlabel('setting')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.savefig('accuracies.png')\n",
    "    plt.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_accuracy_per_setting(results):\n",
    "    \"\"\"\n",
    "    To determine how important each model setting is to overall accuracy,\n",
    "    we'll compute the mean accuracy of all combinations with a particular\n",
    "    setting. For example, compute the mean accuracy of all runs with\n",
    "    min_freq=2.\n",
    "\n",
    "    Params:\n",
    "      results...The output of eval_all_combinations\n",
    "    Returns:\n",
    "      A list of (accuracy, setting) tuples, SORTED in\n",
    "      descending order of accuracy.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    acc_list = defaultdict(lambda:[])\n",
    "    for r in results:\n",
    "        acc = r['accuracy']\n",
    "        acc_list['features=%s' % ' '.join(ft.__name__ for ft in r['features'])].append(acc)\n",
    "        acc_list['min_freq=%s' % r['min_freq']].append(acc)\n",
    "        acc_list['punct=%s' %r['punct']].append(acc)\n",
    "    mean_acc = [(np.mean(a),s) for s,a in acc_list.items()]\n",
    "    return sorted(mean_acc, reverse = True)\n",
    "    pass\n",
    "\n",
    "def fit_best_classifier(docs, labels, best_result):\n",
    "    \"\"\"\n",
    "    Using the best setting from eval_all_combinations,\n",
    "    re-vectorize all the training data and fit a\n",
    "    LogisticRegression classifier to all training data.\n",
    "    (i.e., no cross-validation done here)\n",
    "\n",
    "    Params:\n",
    "      docs..........List of training document strings.\n",
    "      labels........The true labels for each training document (0 or 1)\n",
    "      best_result...Element of eval_all_combinations\n",
    "                    with highest accuracy\n",
    "    Returns:\n",
    "      clf.....A LogisticRegression classifier fit to all\n",
    "            training data.\n",
    "      vocab...The dict from feature name to column index.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    punct = best_result['punct']\n",
    "    tokens_list = [tokenize(d, punct) for d in docs]\n",
    "    feat = best_result['features']\n",
    "    min_freq = best_result['min_freq']\n",
    "    mtrx, vocab = vectorize(tokens_list, feat, min_freq)\n",
    "    clf = LogisticRegression().fit(mtrx, labels)\n",
    "    return clf, vocab\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def top_coefs(clf, label, n, vocab):\n",
    "    \"\"\"\n",
    "    Find the n features with the highest coefficients in\n",
    "    this classifier for this label.\n",
    "    See the .coef_ attribute of LogisticRegression.\n",
    "\n",
    "    Params:\n",
    "      clf.....LogisticRegression classifier\n",
    "      label...1 or 0; if 1, return the top coefficients\n",
    "              for the positive class; else for negative.\n",
    "      n.......The number of coefficients to return.\n",
    "      vocab...Dict from feature name to column index.\n",
    "    Returns:\n",
    "      List of (feature_name, coefficient) tuples, SORTED\n",
    "      in descending order of the coefficient for the\n",
    "      given class label.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    revr_feature = {idx:v for v, idx in vocab.items()}\n",
    "    coef = clf.coef_[0] if label == 1 else -clf.coef_[0]\n",
    "#     print('shape',coef.shape)\n",
    "    result = []\n",
    "    idx = np.argsort(coef)[::-1][:n]\n",
    "    print(idx)\n",
    "    for i in idx:\n",
    "        result.append((revr_feature[i],coef[i]))\n",
    "    return result\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_test_data(best_result, vocab):\n",
    "    \"\"\"\n",
    "    Using the vocabulary fit to the training data, read\n",
    "    and vectorize the testing data. Note that vocab should\n",
    "    be passed to the vectorize function to ensure the feature\n",
    "    mapping is consistent from training to testing.\n",
    "\n",
    "    Note: use read_data function defined above to read the\n",
    "    test data.\n",
    "\n",
    "    Params:\n",
    "      best_result...Element of eval_all_combinations\n",
    "                    with highest accuracy\n",
    "      vocab.........dict from feature name to column index,\n",
    "                    built from the training data.\n",
    "    Returns:\n",
    "      test_docs.....List of strings, one per testing document,\n",
    "                    containing the raw.\n",
    "      test_labels...List of ints, one per testing document,\n",
    "                    1 for positive, 0 for negative.\n",
    "      X_test........A csr_matrix representing the features\n",
    "                    in the test data. Each row is a document,\n",
    "                    each column is a feature.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    test_docs, test_labels = read_data(os.path.join('data', 'test'))\n",
    "    test_tokens = [tokenize(doc,best_result['punct']) for doc in test_docs]\n",
    "    x_test, vocab = vectorize(test_tokens,best_result['features'],best_result['min_freq'],vocab)\n",
    "    return test_docs, test_labels, x_test\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_misclassified(test_docs, test_labels, X_test, clf, n):\n",
    "    \"\"\"\n",
    "    Print the n testing documents that are misclassified by the\n",
    "    largest margin. By using the .predict_proba function of\n",
    "    LogisticRegression <https://goo.gl/4WXbYA>, we can get the\n",
    "    predicted probabilities of each class for each instance.\n",
    "    We will first identify all incorrectly classified documents,\n",
    "    then sort them in descending order of the predicted probability\n",
    "    for the incorrect class.\n",
    "    E.g., if document i is misclassified as positive, we will\n",
    "    consider the probability of the positive class when sorting.\n",
    "\n",
    "    Params:\n",
    "      test_docs.....List of strings, one per test document\n",
    "      test_labels...Array of true testing labels\n",
    "      X_test........csr_matrix for test data\n",
    "      clf...........LogisticRegression classifier fit on all training\n",
    "                    data.\n",
    "      n.............The number of documents to print.\n",
    "\n",
    "    Returns:\n",
    "      Nothing; see Log.txt for example printed output.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    probas = clf.predict_proba(X_test)\n",
    "#     print(probas)\n",
    "    preds = clf.predict(X_test)\n",
    "#     idx = [i if preds[i] != test_labels[i] for i in range(len(test_labels))]\n",
    "    idx = np.where(preds != test_labels)\n",
    "    \n",
    "    print (idx[0])\n",
    "    pre_probs = [probas[i][0] if probas[i][0]> probas[i][1] else probas[i][1] for i in range(len(test_labels))]\n",
    "   \n",
    "    wrong_preds_idx =  sorted(idx[0], key = lambda x: pre_probs[x], reverse = True)[:n]\n",
    "    print(wrong_preds_idx)\n",
    "    for i in wrong_preds_idx:\n",
    "        print('\\ntruth=%d predicted=%d proba=%f\\n%s' % (test_labels[i],preds[i],pre_probs[i],test_docs[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Put it all together.\n",
    "    ALREADY DONE.\n",
    "    \"\"\"\n",
    "    feature_fns = [token_features, token_pair_features, lexicon_features]\n",
    "    # Download and read data.\n",
    "    download_data()\n",
    "    docs, labels = read_data(os.path.join('data', 'train'))[:2]\n",
    "    # Evaluate accuracy of many combinations\n",
    "    # of tokenization/featurization.\n",
    "    results = eval_all_combinations(docs, labels,\n",
    "                                    [True, False],\n",
    "                                    feature_fns,\n",
    "                                    [2,5,10])\n",
    "    # Print information about these results.\n",
    "    best_result = results[0]\n",
    "    worst_result = results[-1]\n",
    "    print('best cross-validation result:\\n%s' % str(best_result))\n",
    "    print('worst cross-validation result:\\n%s' % str(worst_result))\n",
    "    plot_sorted_accuracies(results)\n",
    "    print('\\nMean Accuracies per Setting:')\n",
    "    print('\\n'.join(['%s: %.5f' % (s,v) for v,s in mean_accuracy_per_setting(results)]))\n",
    "\n",
    "    # Fit best classifier.\n",
    "    clf, vocab = fit_best_classifier(docs, labels, results[0])\n",
    "\n",
    "    # Print top coefficients per class.\n",
    "    print('\\nTOP COEFFICIENTS PER CLASS:')\n",
    "    print('negative words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 0, 5, vocab)]))\n",
    "    print('\\npositive words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 1, 5, vocab)]))\n",
    "\n",
    "    # Parse test data\n",
    "    test_docs, test_labels, X_test = parse_test_data(best_result, vocab)\n",
    "\n",
    "    # Evaluate on test set.\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('testing accuracy=%f' %\n",
    "          accuracy_score(test_labels, predictions))\n",
    "\n",
    "    print('\\nTOP MISCLASSIFIED TEST DOCUMENTS:')\n",
    "    print_top_misclassified(test_docs, test_labels, X_test, clf, 5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation result:\n",
      "{'punct': False, 'features': (<function token_features at 0x7ffa4d188f28>, <function lexicon_features at 0x7ffa4d195bf8>), 'min_freq': 5, 'accuracy': 0.71999999999999997}\n",
      "worst cross-validation result:\n",
      "{'punct': True, 'features': (<function token_pair_features at 0x7ffa4d1957b8>,), 'min_freq': 5, 'accuracy': 0.58250000000000002}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAAOtCAYAAACv1xlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWuX/AytD5ay0RUOvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa87a03f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracies per Setting:\n",
      "features=token_features lexicon_features: 0.70833\n",
      "features=token_features: 0.70375\n",
      "min_freq=2: 0.66089\n",
      "punct=False: 0.66024\n",
      "features=token_features token_pair_features lexicon_features: 0.65833\n",
      "min_freq=5: 0.65804\n",
      "features=lexicon_features: 0.65500\n",
      "punct=True: 0.65333\n",
      "min_freq=10: 0.65143\n",
      "features=token_features token_pair_features: 0.64250\n",
      "features=token_pair_features lexicon_features: 0.63292\n",
      "features=token_pair_features: 0.59667\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Put it all together.\n",
    "    ALREADY DONE.\n",
    "    \"\"\"\n",
    "    feature_fns = [token_features, token_pair_features, lexicon_features]\n",
    "    # Download and read data.\n",
    "    download_data()\n",
    "    docs, labels = read_data(os.path.join('data', 'train'))[:2]\n",
    "    # Evaluate accuracy of many combinations\n",
    "    # of tokenization/featurization.\n",
    "    results = eval_all_combinations(docs, labels,\n",
    "                                    [True, False],\n",
    "                                    feature_fns,\n",
    "                                    [2,5,10])\n",
    "    # Print information about these results.\n",
    "    best_result = results[0]\n",
    "    worst_result = results[-1]\n",
    "    print('best cross-validation result:\\n%s' % str(best_result))\n",
    "    print('worst cross-validation result:\\n%s' % str(worst_result))\n",
    "    plot_sorted_accuracies(results)\n",
    "    print('\\nMean Accuracies per Setting:')\n",
    "    print('\\n'.join(['%s: %.5f' % (s,v) for v,s in mean_accuracy_per_setting(results)]))\n",
    "\n",
    "    # Fit best classifier.\n",
    "    clf, vocab = fit_best_classifier(docs, labels, results[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP COEFFICIENTS PER CLASS:\n",
      "negative words:\n",
      "shape (1741,)\n",
      "[   0 1045 1656  898 1719]\n",
      "neg_words: 1.11751\n",
      "token=nothing: 0.79516\n",
      "token=waste: 0.63287\n",
      "token=looks: 0.55395\n",
      "token=worst: 0.54712\n",
      "\n",
      "positive words:\n",
      "shape (1741,)\n",
      "[1720  798    1 1393 1296]\n",
      "token=worth: 0.49498\n",
      "token=its: 0.49163\n",
      "pos_words: 0.48652\n",
      "token=something: 0.48596\n",
      "token=scene: 0.47207\n",
      "testing accuracy=0.777500\n"
     ]
    }
   ],
   "source": [
    "    # Print top coefficients per class.\n",
    "    print('\\nTOP COEFFICIENTS PER CLASS:')\n",
    "    print('negative words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 0, 5, vocab)]))\n",
    "    print('\\npositive words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 1, 5, vocab)]))\n",
    "\n",
    "    # Parse test data\n",
    "    test_docs, test_labels, X_test = parse_test_data(best_result, vocab)\n",
    "\n",
    "    # Evaluate on test set.\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('testing accuracy=%f' %\n",
    "          accuracy_score(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP MISCLASSIFIED TEST DOCUMENTS:\n",
      "[  0   2   8  12  16  20  30  35  39  46  49  50  52  54  55  56  61  66\n",
      "  73  75  76  81  88  90  92  94  95 105 110 111 113 119 121 126 129 133\n",
      " 146 147 158 160 161 165 184 196 199 207 209 215 218 220 222 223 224 238\n",
      " 240 247 252 253 254 259 264 266 280 287 295 297 298 303 305 313 314 317\n",
      " 321 323 324 325 329 333 342 343 347 348 353 356 365 369 375 381 395]\n",
      "[333, 209, 356, 259, 119]\n",
      "\n",
      "truth=1 predicted=0 proba=0.999983\n",
      "This movie blew me away - I have only seen two episodes of the show, never saw the first movie, but went to a pre-screening where Johnny Knoxville himself introduced the movie, telling us to 'turn off our sense of moral judgment for an hour and a half.' He was right. As a movie, this would probably rate a 2, given it has zero plot, no structure besides randomness, and very little production value. However, that isn't the point. Everyone in our theatre was laughing and gasping the whole way through - not only were some of the stunts creative (see trailer if you need to know but they hid some of the best (or worst depending on how you want to look at it)), but some of the stuff they did took us completely by surprise. These guys do some stuff that won't make it into your newspaper reviews (and probably can't even be published here), involving lots of things below the belt. However, almost 3/4 of the stunts are fantastically hysterical (even if morally condemnable, but remember Knoxville's statement), and if you are in the right mindset this movie is hysterical to watch. Only about 20 minutes of this movie could have actually been shown on TV, so consider yourself warned of what you're getting into - some stuff is disgusting, but instead of being repulsed by it you end up laughing at the sheer stupidity of it all. As a person who thought Jackass the TV show was an over-hyped fad with only a few funny sketches and lots of unnecessary pain, the amount of fun I had at this movie has made me realize that having no boundaries is the best environment for these guys to work in. It's a lot of fun and should be a great comedic fix until the Borat movie comes out. With this movie, you may think you know what you're getting, but these guys are a few steps ahead of you - I guarantee you'll be surprised by the 3rd sketch. So enjoy, and don't worry: you won't want to perform almost any of their stuff at home.\n",
      "\n",
      "truth=0 predicted=1 proba=0.998058\n",
      "Macbeth is one of the most frequently told stories in cinema and has been translated many times in numerous theater and celluloid settings. Originally written by William Shakespeare in the early 1600's, Macbeth tells the story of betrayal among royalty and one man's quest for power. Director Geoffrey Wright (Romper Stomper) tries his hand at updating Macbeth by setting it in the contemporary Melbourne underworld. A film where the characters substitute swords for guns (ala Baz Luhrman's Romeo and Juliet) and royal vassals for gangsters, Macbeth is a gritty, violent, but critically flawed film.<br /><br />Macbeth (Sam Worthington)works for King Duncan (Gary Sweet). After being elevated to the Thane of Glamis by The King (as was prophesied to Macbeth by three witches), Macbeth starts setting his eyes on the throne. One night the King comes to stay at Macbeth's house and Lady Macbeth (Victoria Hill) talks him into killing The King to assume power. Macbeth kills his master and then assumes his crown. But success has it's downside, as Macbeth soon finds out, when he has to go to hideous lengths to protect his murderous secret. <br /><br />OK, first things first. The film's major fault is Sam Worthington. His portrayal of Macbeth is in a word... boring. I honestly didn't care about Macbeth while watching the film. I had more sympathy for Victoria Hill's Lady Macbeth because she bothered to act at least. Worthington sits sullen and wood faced throughout the entire film. I felt like he was doing his best impression of Johnny Deep's George Jung character from Blow... but without the charisma. I have never seen Worthington in a film before so I'm not sure if it was his or the Director's fault, but either way the glue that should have tied everything together into one cohesive unit is weak.<br /><br />The dialog is good, but when matched up to the Geoffrey Wright's Australian Gangster Motif seems a bit out of place. Frentically paced action sequences mixed with long Shaksperian musings creates pacing conflict within the film. I understand that this is Macbeth and that the director wanted to use the original dialog intact. But hard, fast action scenes following a three minute soliloquy tends to get annoying if not a bit pretentious. <br /><br />The camera-work is highly stylized, and for the most part, it works well. One thing that I found annoying was how the camera would slowly jostle back and forth, almost constantly. I don't mind shots like that it's just overdone. It's passes beyond the realm of being cool and stylish and instead becomes irritating. Other than that, the art direction and cinematography is fairly well-done. <br /><br />For all of the good qualities Macbeth possesses; stylish direction, Shakespearian dialog, a strong soundtrack, supernatural nude witches(the weird sisters), and good helpings of brutal, bloody violence. All of these strengths are forgotten when one considers Sam Worthington's uninspired portrayal of Macbeth. The role of Macbeth was essential for tying everything together and in this respect Geoffrey Wright and Sam Worthington failed miserably, making Macbeth a forgettable foray into Shakespeare.\n",
      "\n",
      "truth=0 predicted=1 proba=0.997010\n",
      "Two stars <br /><br />Amanda Plummer looking like a young version of her father, Christopher Plummer in drag, stars in this film along with Robert Forster--who really should have put a little shoe black on top of that bald spot.<br /><br />I've never seen Amanda Plummer in a good film. She always plays these slightly wacky characters in films that don't quite add up, and she does so yet again in this one.<br /><br />Firstly, we have two young women, sisters, who don't resemble in the slightest, who allow themselves to be picked up, separately, by questionable men along the roadways.<br /><br />Amanda's character, Sandra, does at least have a good reason for allowing Dr. Jake (Robert Forster)to pick her up in the first place. She has been run off the road, seemingly by a maniac, and her car is pretty much destroyed.<br /><br />Warning - Spoilers ahead! <br /><br />However, as we go along, we realize Dr. Jake is not playing with a full deck any more than Amanda is. He makes every decision based on the flip of a coin.<br /><br />When Dr. Jake and Amanda arrive at a motel, who do we see but the maniac's car, and what does Amanda do but get inside his station wagon and start snooping around. What her motive was for doing this is never clear considering the man is apparently dangerous and might try to kill her. One would think the last thing she would do is place herself in such a precarious situation.<br /><br />Not only does she snoop around, but she finds some money and takes it.<br /><br />Shortly after this we have several other things that don't add up.<br /><br />Dr. Jake, with Amanda as his passenger, runs out of gas, and the two of them abandon his car and begin walking. One would think crossing a desert, he would have checked his gas gauge--this seems a very unlikely thing for him to allow to happen. Then later, he is seen driving the same car. When or how did he get the car back? <br /><br />Dr. Jake tells Amanda he knows she has taken the money. Now how would he know that? He didn't see her do it as far as I know, and she didn't tell him she did it.<br /><br />Then later we have a character named Santini (David Thewlis), the man who was driving the station wagon, give the two of them a lift and I'll be darned if he doesn't know Amanda took the money as well. How would he know that? <br /><br />It loses credibility at an alarming rate the further we go.<br /><br />When Alice, Amanda's sister (Fairuza Balk)gets in the clutches of the killer and decides her fate on the toss of a coin - one would think she would be very, very careful that the coin she swaps for a trick coin would definitely be the trick coin - but apparently it isn't.<br /><br />It's jarring things like this, that destroy any credibility this movie may have had.\n",
      "\n",
      "truth=0 predicted=1 proba=0.995601\n",
      "Steven Seagal is back! Here with his third film released this year. Of course as a one time fan who has become increasingly disgruntled I can say it comes as no surprise that this is pretty lame. Firstly the film made headlines because of apparent problems in production due to Seagal. He would turn up late on set, change the script, crew etc and generally cause problems for the director, Don E Faun Leroy (his lack of talent his trouble enough!). This also happens in their second collaboration the upcoming Mercenary which promises to be just as bad as this garbage. This also marks a big turning point in Seagal's career because this film is the first of his to really dig out the stock footage. There was a little in Ticker but this film takes the biscuit. They borrow bits from, The Order (A Van Damme movie, Seagals biggest rival in DTV movies!) No Code Of Conduct, Undisputed, and also an entire action sequence from the little known Peter Weller starring vehicle Top Of The World. Interestingly the car chase stolen from Weller's epic, made almost 10 years ago and ironically probably cheaper than this garbage, is actually by far the best action scene of the film. I was shocked enough when Dolph Lundgren had a brief stint in the stock action video world, which thankfully he has escaped from. Seagal though is the leader of the DTV action market currently, with Van Damme and Snipes his main rivals. Seagal still manages to sell movies and for the life of me I don't know how. Surely the fans must be getting bored of this awfulness, longing for a return to the likes of Above The Law. The story here is totally lame. In fact the film has so many plot holes it doesn't bare thinking about. For example at the end of a film there's a little girl that Seagal apparently knows at an orphanage who he gives a necklace to. Why I don't know but we never see her at all in the rest of the movie, or hear her mentioned. Seagal has a girlfriend in this movie who at the beginning of the film is with a psychic and she becomes haunted by visions, which by the end of the film are never explained and mean nothing. The film is so ridiculously glued together by a series of meaningless pap that it becomes headache inducing.. This is by far Seagal's dumbest movie! Seagal himself is as wooden as ever, however to his credit he doesn't get dubbed in this one as far as I could tell. Seagal does however feel the need to talk like he is a gangsta rapper, making me long for the days he would don his Brooklyn/Italian-American accent, in his classic early films. He also has a painfully unfunny double act with Treach, who I assume is a rapper. It is funny how producers seem to think that the combo of Seagal, plus hip-hop star seems to work, because his team up with DMX in Exit Wounds was his most successful film since Under Siege. Clearly though if no one has heard of the rapper, it won't work. This is an action film though and so the action itself must be judged. Unfortunately the action that didn't come form the NU Image back catalogue is strictly routine. There are a few small fight scenes with some classic Seagal aikido but when 90% is performed by his stunt double, who really does have a rigorous work out in this film, it really doesn't impress much. There are also some standard gun fights which really only have some nice violent and bloody squibbage going for them. All in all this is a painfully boring experience and once again I'm left giving the same verdict: Seagal has lost it! I keep asking the years old question now, \"why do people still watch his movies?\" That is all very well and good as a question but the sad bastard that I am continues to watch his films in the deluded hope he may do something good once again. Chances are slim, unlike Seagal's ever expanding waistline. *1/2\n",
      "\n",
      "truth=0 predicted=1 proba=0.993250\n",
      "I have to agree that the movie is not the best I've ever seen, but I would like to make mention that the actors portraying Tommy and Jimmy Dorsey were the actual Dorsey brothers. As actors, they were wonderful musicians. The movie, based on their famous split, would have been better had professional actors played the parts. Many movies made during this time frame took advantage of the popularity of Big Bands. Most often, the movies were not that good because musicians are not actors by trade. Most of the movie-going audience didn't go to see Tommy or Jimmy Dorsey playing themselves; they went for the plot and the music. I've never been much of a Dorsey fan, but the music is good even today.<br /><br />I have to comment on a previous post regarding the actors who played Mom and Pop Dorsey and that their accents would be considered extreme by a Dublin audience. Arthur Shields and Sara Allgood were actually Irish actors, both born in Dublin. You might remember Mr. Shields as the Reverend Mr. Playfair in The Quiet Man and Ms. Allgood as Mrs. Monahan in Cheaper By The Dozen.\n"
     ]
    }
   ],
   "source": [
    "    print('\\nTOP MISCLASSIFIED TEST DOCUMENTS:')\n",
    "    print_top_misclassified(test_docs, test_labels, X_test, clf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
