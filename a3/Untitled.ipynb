{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Assignment 3:  Recommendation systems\n",
    "\n",
    "### Here we'll implement a content-based recommendation algorithm.\n",
    "### It will use the list of genres for a movie as the content.\n",
    "### The data come from the MovieLens project: http://grouplens.org/datasets/movielens/\n",
    "\n",
    "### Please only use these imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\" DONE. Download and unzip data.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/h9ubx22ftdkyvd5/ml-latest-small.zip?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'ml-latest-small.zip')\n",
    "    zfile = zipfile.ZipFile('ml-latest-small.zip')\n",
    "    zfile.extractall()\n",
    "    zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    tokens = [tokenize_string(t) for t in movies['genres'].tolist()]\n",
    "    movies['tokens']=tokens\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "movies = pd.DataFrame([[123, 'Horror|Horror|Romance'], [456, 'Sci-Fi'], [789, 'Sci-Fi|Drama']], columns=['movieId', 'genres'])\n",
    "movies = tokenize(movies)\n",
    "movies['tokens'].tolist()\n",
    "vocab_keys = sorted(set(sum(movies['tokens'].tolist(),[])))\n",
    "print (vocab_keys)\n",
    "vocabulary = defaultdict(lambda: len(vocabulary))\n",
    "for key in vocab_keys:\n",
    "    vocabulary[key]\n",
    "print (dict(vocabulary))\n",
    "vocab_values = [v for v in range(len(vocab_keys))]\n",
    "vocab = dict(zip(vocab_keys,vocab_values))\n",
    "count = Counter(sum(movies['tokens'].tolist(),[]))\n",
    "for m in movies.iterrows():\n",
    "    print (m[1]['movieId'])\n",
    "    c = Counter(m[1]['tokens'])\n",
    "    print (c,c['horror'])\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featurize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    vocab_keys = sorted(set(sum(movies['tokens'].tolist(),[])))\n",
    "    # print (vocab_keys)\n",
    "    \n",
    "#     vocab_values = [v for v in range(len(vocab_keys))]\n",
    "#     vocab = dict(zip(vocab_keys,vocab_values))\n",
    "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
    "    for key in vocab_keys:\n",
    "        vocabulary[key]\n",
    "    vocab = dict(vocabulary)\n",
    "    num_d = movies.shape[0]\n",
    "    features = []\n",
    "    for m in movies.iterrows():\n",
    "        m_tokens = m[1]['tokens']\n",
    "        c = Counter(m_tokens)\n",
    "#         print(c.items())\n",
    "        tf = c.values()\n",
    "        max_tf = c.most_common(1)[0][1]\n",
    "#         print (tf)\n",
    "        df = len(c.items())\n",
    "        temp = max_tf*np.log10(num_d/df)\n",
    "        tfidf = [t/temp for t in tf]\n",
    "        col_idx = [vocab[f] for f in c.keys()]\n",
    "        data = np.zeros(len(vocab))\n",
    "        data[col_idx] = tfidf\n",
    "        features.extend(csr_matrix(data))\n",
    "#         print(csr_matrix(data).toarray())\n",
    "    movies['features'] = features\n",
    "    return (movies, vocab)\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "   (0, 1)\t5.67887358727\n",
      "  (0, 2)\t2.83943679363\n",
      "value   (0, 1)\t5.67887358727\n",
      "  (0, 2)\t2.83943679363\n",
      "type <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "movies = pd.DataFrame([[123, 'Horror|Horror|Romance'], [456, 'Sci-Fi'], [789, 'Sci-Fi|Drama']], columns=['movieId', 'genres'])\n",
    "movies = tokenize(movies)\n",
    "movies,vocab = featurize(movies)\n",
    "features = movies.loc[0]['features']\n",
    "f = features[0]\n",
    "print('features\\n',features)\n",
    "print('value', f)\n",
    "# print('vocab',vocab)\n",
    "print('type', type(f))\n",
    "# cosine_sim(features[2],features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = [ 0, 5.67887359, 2.83943679, 0]\n",
    "v2 = [ 5.67887359,0, 0, 5.67887359]\n",
    "np.dot(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    \"\"\"DONE.\n",
    "    Returns a random split of the ratings matrix into a training and testing set.\n",
    "    \"\"\"\n",
    "    test = set(range(len(ratings))[::1000])\n",
    "    train = sorted(set(range(len(ratings))) - test)\n",
    "    test = sorted(test)\n",
    "    return ratings.iloc[train], ratings.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two 1-d csr_matrices.\n",
    "    Each matrix represents the tf-idf feature vector of a movie.\n",
    "    Params:\n",
    "      a...A csr_matrix with shape (1, number_features)\n",
    "      b...A csr_matrix with shape (1, number_features)\n",
    "    Returns:\n",
    "      The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\n",
    "      where ||a|| indicates the Euclidean norm (aka L2 norm) of vector a.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    v1 = a.toarray()\n",
    "    v2 = b.toarray()\n",
    "    norms = np.linalg.norm(v1)*np.linalg.norm(v2)\n",
    "    return np.dot(v1[0],v2[0])/norms\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70710678118654746"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(features[2],features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    \"\"\"\n",
    "    Using the ratings in ratings_train, predict the ratings for each\n",
    "    row in ratings_test.\n",
    "\n",
    "    To predict the rating of user u for movie i: Compute the weighted average\n",
    "    rating for every other movie that u has rated.  Restrict this weighted\n",
    "    average to movies that have a positive cosine similarity with movie\n",
    "    i. The weight for movie m corresponds to the cosine similarity between m\n",
    "    and i.\n",
    "\n",
    "    If there are no other movies with positive cosine similarity to use in the\n",
    "    prediction, use the mean rating of the target user in ratings_train as the\n",
    "    prediction.\n",
    "\n",
    "    Params:\n",
    "      movies..........The movies DataFrame.\n",
    "      ratings_train...The subset of ratings used for making predictions. These are the \"historical\" data.\n",
    "      ratings_test....The subset of ratings that need to predicted. These are the \"future\" data.\n",
    "    Returns:\n",
    "      A numpy array containing one predicted rating for each element of ratings_test.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    num = ratings_test.shape[0]\n",
    "    pre_rating = np.zeros(num)\n",
    "    rate_mean = defaultdict(lambda: -1)\n",
    "    for i in range(num):\n",
    "        r_test = ratings_test.iloc[i]\n",
    "        user = r_test['userId']\n",
    "        pre_movie = r_test['movieId']\n",
    "        pre_feature = movies[(movies.movieId == pre_movie)]['features'].item()\n",
    "#         print('pre',pre_feature.item())\n",
    "#         print('type',type(pre_feature.item()))\n",
    "        u_ratings_train = ratings_train[ratings_train.userId == user]\n",
    "        train_movies = u_ratings_train['movieId']\n",
    "        if user not in rate_mean:\n",
    "            avg_rating = u_ratings_train['rating'].mean()\n",
    "            rate_mean[user]=avg_rating\n",
    "#         cslist = []\n",
    "        sum_rating = 0\n",
    "        sum_cos = 0\n",
    "        for u_m in train_movies:\n",
    "            train_feature = movies[(movies.movieId == u_m)]['features'].item()  \n",
    "            rating = u_ratings_train[u_ratings_train.movieId == u_m]['rating'].item()\n",
    "            sum_rating += cosine_sim(pre_feature, train_feature)*rating\n",
    "            sum_cos += cosine_sim(pre_feature, train_feature)\n",
    "        if sum_cos is 0:\n",
    "            pre = -1\n",
    "        else:\n",
    "            pre = sum_rating/sum_cos\n",
    "#             print(movies[(movies.movieId == u_m)])\n",
    "#             train_feature = movies[(movies.movieId == u_m)]['features'].item()       \n",
    "#             cslist.append((u_m, cosine_sim(pre_feature, train_feature)))\n",
    "#         pre_m = sorted(cslist, key=lambda x: x[1], reverse=True)[0]\n",
    "#         print(pre)\n",
    "        if pre > 0:\n",
    "            pre_rating[i] =pre\n",
    "        else:\n",
    "            pre_rating[i] = rate_mean[user]\n",
    "#     print('mean of users:',rate_mean.items())\n",
    "    return pre_rating\n",
    "    pass  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('action', 0), ('adventure', 1), ('animation', 2), ('children', 3), ('comedy', 4), ('crime', 5), ('documentary', 6), ('drama', 7), ('fantasy', 8), ('film-noir', 9)]\n",
      "99903 training ratings; 101 testing ratings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cong/.local/lib/python3.5/site-packages/ipykernel/__main__.py:51: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error=0.789847\n",
      "[ 2.64185496  2.57464639  2.73540562  4.1070431   3.2113471   4.18485263\n",
      "  3.94769278  3.95200736  3.30050115  3.54732792]\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error(predictions, ratings_test):\n",
    "    \"\"\"DONE.\n",
    "    Return the mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    return np.abs(predictions - np.array(ratings_test.rating)).mean()\n",
    "\n",
    "\n",
    "def main():\n",
    "    download_data()\n",
    "    path = 'ml-latest-small'\n",
    "    ratings = pd.read_csv(path + os.path.sep + 'ratings.csv')\n",
    "    movies = pd.read_csv(path + os.path.sep + 'movies.csv')\n",
    "    movies = tokenize(movies)\n",
    "    movies, vocab = featurize(movies)\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "    print('error=%f' % mean_absolute_error(predictions, ratings_test))\n",
    "    print(predictions[:10])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('a', 0),('b',-1),('c', 1),('d', 0),('e', 0)]\n",
    "sorted(data, key=lambda x: x[1],reverse = True)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean of users: dict_items([(1.0, 2.5526315789473686), (516.0, 3.4594594594594597), (262.0, 2.6111111111111112), (520.0, 3.3823529411764706), (266.0, 3.736842105263158), (15.0, 2.622791519434629), (17.0, 3.7430939226519335), (275.0, 4.4601990049751246), (529.0, 3.5290215588723051), (533.0, 3.3518518518518516), (22.0, 3.2739726027397262), (28.0, 4.2653061224489797), (285.0, 3.1638954869358669), (30.0, 3.7658415841584159), (543.0, 4.333333333333333), (547.0, 3.3660527417329424), (292.0, 3.9566787003610107), (550.0, 3.5820895522388061), (41.0, 3.8661616161616164), (301.0, 3.3409090909090908), (559.0, 4.4140625), (306.0, 3.408385093167702), (564.0, 3.5535905680600215), (311.0, 3.0068762278978389), (58.0, 3.3888888888888888), (315.0, 2.4814814814814814), (575.0, 3.3919413919413919), (608.0, 3.9491525423728815), (580.0, 3.271986970684039), (71.0, 4.2727272727272725), (73.0, 3.3738346799254195), (330.0, 3.4680851063829787), (75.0, 3.2777777777777777), (592.0, 3.8787878787878789), (83.0, 3.9281250000000001), (597.0, 4.0199004975124382), (345.0, 3.73943661971831), (91.0, 4.1677852348993287), (570.0, 3.6574803149606301), (367.0, 3.3425925925925926), (353.0, 2.5942307692307693), (102.0, 3.9748892171344163), (358.0, 3.1866883116883118), (97.0, 3.0275590551181102), (617.0, 3.1216216216216215), (111.0, 3.5073529411764706), (50.0, 3.2888888888888888), (119.0, 3.5015624999999999), (378.0, 3.2328767123287672), (380.0, 3.365819209039548), (637.0, 4.166666666666667), (127.0, 4.0499999999999998), (384.0, 3.2334710743801653), (388.0, 3.6472819216182049), (133.0, 2.3785310734463279), (649.0, 3.50561797752809), (394.0, 2.7965686274509802), (654.0, 4.0712000000000002), (664.0, 3.7953667953667956), (146.0, 3.5555555555555554), (405.0, 3.6545012165450124), (152.0, 3.4308755760368665), (411.0, 3.7307692307692308), (671.0, 3.9166666666666665), (624.0, 2.8958453548759375), (162.0, 3.4137931034482758), (422.0, 3.9761904761904763), (170.0, 2.54), (427.0, 3.8890784982935154), (433.0, 3.8299492385786804), (182.0, 3.8153846153846156), (585.0, 4.2474916387959869), (442.0, 4.2321428571428568), (189.0, 2.6914285714285713), (452.0, 3.1893203883495147), (198.0, 3.7905405405405403), (457.0, 2.5028089887640448), (460.0, 3.7155477031802122), (205.0, 3.4121951219512194), (463.0, 3.3589211618257262), (212.0, 3.1080000000000001), (213.0, 2.6617161716171616), (470.0, 3.3014705882352939), (472.0, 3.7882991556091676), (220.0, 3.5555555555555554), (479.0, 4.0510204081632653), (483.0, 3.5847457627118646), (294.0, 3.5845665961945032), (232.0, 3.9662261380323054), (468.0, 2.9666666666666668), (239.0, 3.7068965517241379), (497.0, 3.8076923076923075), (243.0, 3.3954248366013071), (505.0, 3.2715736040609138), (250.0, 4.2727272727272725), (509.0, 3.3671366594360088), (605.0, 3.0458715596330275)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pop</th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.7</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.6</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.4</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pop   state  year\n",
       "0  1.5    Ohio  2000\n",
       "1  1.7    Ohio  2001\n",
       "2  3.6    Ohio  2002\n",
       "3  2.4  Nevada  2001\n",
       "4  2.9  Nevada  2002"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "        'year': [2000, 2001, 2002, 2001, 2002],\n",
    "        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n",
    "frame = pd.DataFrame(data)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[frame.year== 2000]['pop'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-303-0cb9db621b20>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-303-0cb9db621b20>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for f in frame.iterater/:\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for f in frame.iterater:\n",
    "    print(f['pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
